{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Session 3](#1.-Session-3)\n",
    "\t* [1.1 Previous challenges](#1.1-Previous-challenges)\n",
    "\t* [1.2 Installing other packages with `conda` or `pip`](#1.2-Installing-other-packages-with-conda-or-pip)\n",
    "* [2. Astropy continued](#2.-Astropy-continued)\n",
    "\t* [2.1 Astroquery](#2.1-Astroquery)\n",
    "\t* [2.2 Finding points within a spherical angular distance](#2.2-Finding-points-within-a-spherical-angular-distance)\n",
    "\t* [2.3 SciServer](#2.3-SciServer)\n",
    "\t* [2.4 Cosmology](#2.4-Cosmology)\n",
    "* [3. AstroML](#3.-AstroML)\n",
    "* [4. HDF5 with `pytables` or `h5py`](#4.-HDF5-with-pytables-or-h5py)\n",
    "\t* [4.1 A note about `try...finally`](#4.1-A-note-about-try...finally)\n",
    "* [5. Reading binary files with `fromfile`](#5.-Reading-binary-files-with-fromfile)\n",
    "* [6. Pandas](#6.-Pandas)\n",
    "* [7. Healpy (using HealPix)](#7.-Healpy-%28using-HealPix%29)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RUN THIS FIRST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Session 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to cover one or two more features of Astropy and introduce some packages from outside the standard library that might be useful. If you're comfortable with the material covered in previous sessions, you can try to install and explore these packages for yourself. If not, then carry on working through the earlier notebooks and/or applying what you've learned to your own projects.\n",
    "\n",
    "Please share any other useful knowledge/techniques/packages you might have come across!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Previous challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A solution to the challenge to match lists of integers using `numpy.searchsorted` is in the file `match.py`. In the same file I've included some routines to test the function. The last of these test routines generates huge random unsorted lists of integers to time how long the match takes. That module can be run from the command line (in which case Python runs the block under `if __name__ == '__main__'`) passing the lengths of the two lists to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run match.py 10000 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another challenge was to reproduce a fancy 2D histogram of a projected particle distribution with 1D histograms along each axis. The solution to this is in `w2challenge_dotplot.py`. This plots all three possible projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run w2challenge_dotplot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Installing other packages with `conda` or `pip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a long section. Bottom line: I recommend using `conda` to install packages if you can.*\n",
    "\n",
    "To install some of the packages below, you will need to use either `conda` or `pip`. `conda` is a tool for creating self-contained 'environments' in which you can install software separately from anything else on the system. Miniconda and Anaconda are 'distributions' (collections) of python packages that rely on `conda` to install those packages: the distributions provide the packages, and you use the `conda` tool to manage them. `conda` is not specifically linked to Python (more on this below).\n",
    "\n",
    "`pip` is a simpler idea than `conda`. More or less, `pip` is a tool to install *only* Python packages. `pip` knows about  one big official archive of Python packages on the web (rather than the various 'distributions', like Anaconda, that `conda` has access to). Unlike `conda`, `pip` will try to to install packages in a default system location, which usually needs the root password to write to. If you are working on someone else's machine or want to keep packages out of the system python directory, you can ask `pip` to install to a specific directory under your control, or to a default location in your `/home` directory (examples below).\n",
    "\n",
    "In the past almost everyone used `pip`. Now more people are using `conda`: one problem with `conda` is that the learning curve is slightly steeper than `pip`, because of the idea of 'environments' in `conda`. The advantage is that `conda` can install a wider range of software, and can isolate everything very cleanly from the version of Python used by the operating system. The advantage of `pip` is that it is less hassle and is more-or-less built in to Python (`conda` has to be installed separately, almost always this is done by installing Miniconda or another distribution that relies on `conda`.\n",
    "\n",
    "*** To install packages with `conda`: ***\n",
    "\n",
    "`conda install [name of package]`\n",
    "\n",
    "To make a `conda` environment called `myenvironment` that initially has the packages `numpy`, `matplotlib` and `astropy` in it you can do:\n",
    "\n",
    "`conda create myenvironment numpy matplotlib astropy`\n",
    "\n",
    "Then you need to activate that environment when you work: `source activate myenvironment`. The shell prompt will change to show you're working in that environment. From that point on, you will be working with a Python executable and set of Python packages that is isolated to that environment and under your control, independent of any other environments and the system python. \n",
    "\n",
    "To list the packages available in the environment, you can do `conda list`. If you install new packages inside an environment, they will only be available inside that environment. You can have separate environments for Python 2, Python 3 and any other particular version of Python you want. You can use `pip` inside a `conda` environment too, and the effect will be to install the package in that environment, not system-wide.\n",
    "\n",
    "When you're finished working with that environment you can call `source deactivate` to return to the normal system prompt.\n",
    "\n",
    "This seems like a lot of hassle at first, but it does make things easier in the long run.\n",
    "\n",
    "*** To install packages with `pip`: ***\n",
    "\n",
    "`pip install [name of package]`\n",
    "\n",
    "Add the `--user` option if you don't have/want to use root access to install to the system python directory.\n",
    "\n",
    "You can also use the option `--prefix=/some/path/on/your/disk` to install a package to a specific location. Remember that this location will have to be included in your PYTHONPATH shell environment variable (or otherwise added into the list `sys.path` by your Python code) before you can `import` the package.\n",
    "\n",
    "** Non-python dependencies **\n",
    "\n",
    "Some python packages might depend on non-python code. For example, the packages that work with the HDF5 format will require your system to have the HDF5 libraries installed, and the `healpy` package requires the `HEALPIX` libraries. Conda is particularly helpful because it can also install any non-python libraries and software that a given python package depends on. `pip` only does this 'sometimes' for the bigger, better-supported packages (for example, installing `numpy` involves compiling some well-tested, high-performance numerical libraries written in C and Fortran, but this is handled by `pip`). Both HDF5 and HEALPIX are handled by `pip`.\n",
    "\n",
    "If you don't use `conda` but do use linux, then these external dependencies can usually be installed easily with your package manager. For macs there are third-party package managers like [Homebrew](http://brew.sh/). \n",
    "\n",
    "Conda will nearly always install these third-party libraries for you automatically -- my suggestion is to use `conda` for an easy life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Astropy continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we covered:\n",
    "- coordinate conversions using `astropy.coordinates.SkyCoord`\n",
    "- using the `astropy.table.Table` class to read FITS tables\n",
    "\n",
    "This week we'll look at:\n",
    "- using the `astroquery` add-on package to `astropy` to get data from Simbad and SDSS (and other databases)\n",
    "- finding neighbours of points on the sky\n",
    "- a quick note on the cosmology tools in `astropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import astropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Astroquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Astroquery](http://astroquery.readthedocs.io/en/latest/) is a package that can communicate over the internet with various astronomy databases, including the Simbad service to turn names of astronomical objects into (RA,DEC) coordinates, and the Sloan Digital Sky Survey.\n",
    "\n",
    "This is an 'affiliated package' of the Astropy project. According to the webpage, it can be installed as follows with `conda`: `conda install -c astropy astroquery`. The `-c astropy` option tells `conda` to use the Astropy project channel to install the `astroquery` package. Channels are online repositories of packages: Anaconda is one such repository, but there are others, including Astropy. Different repositories provide different collections of packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once `astroquery` is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import astroquery # When you first do this, you might get a harmless warning about the configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the `astropy` coordinates and units modules for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import astropy.coordinates as c\n",
    "import astropy.units as u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGC 2419 is an unusual Milky Way globular cluster. This example shows how to use `astroquery` to find the coordinates of the cluster, and then download nearby sources in SDSS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, use a [Simbad](http://simbad.u-strasbg.fr/simbad/) query to find the coordinates of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import astroquery.simbad\n",
    "simbad  = astroquery.simbad.Simbad() # get a query manager object\n",
    "ngc2419 = simbad.query_object('NGC 2419') # carry out the query and store the result\n",
    "ngc2419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract RA and DEC from this table and convert them to an `astropy` coordinate object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngc2419_pos = c.SkyCoord(ngc2419['RA'],ngc2419['DEC'],unit=(u.hourangle,u.degree))\n",
    "print(ngc2419_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this position for another query, this time to SDSS. We'll download all the entries in the photometric detection table (`photoobj`) within 10 arcminutes of the position of the cluster. We'll ask for position (ra and dec), the 'g' and 'r' band PSF magnitudes (the appropriate magnitudes to use for for stars), the SDSS source class (`type`) and the standard set of SDSS identifiers (objid, run, rerun, camcol and field) which are not used in this example, but which would be useful if we wanted to carry out further queries, e.g. for spectra or additional properties). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import astroquery.sdss\n",
    "sdss = astroquery.sdss.SDSS()\n",
    "res  = sdss.query_region(ngc2419_pos,radius=10*u.arcmin,\n",
    "                       photoobj_fields=['ra','dec','objid','run','rerun','camcol','field',\n",
    "                                        'psfMag_g', 'psfMag_r','type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a quick dotplot of the sources to check we have something reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(4,4))\n",
    "pl.scatter(res['ra'],res['dec'],s=1,edgecolor='None');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is an 'empty' ring around the cluster position (the centre of the field) with no points in it. This is because the SDSS photometry pipeline fails in very crowded fields. The second empty region to the 'left' (in this case, west) of the cluster is because there's a very bright star there, which also limits the SDSS pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get an image of the cluster. In this example we'll use the SDSS g band. For this we use a SkyView query: we could have used SDSS.get_images, but SkyView can join together multiple SDSS fields into a single larger image -- this would be much harder to do if we got the individual images from SDSS ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import astroquery.skyview\n",
    "skyview = astroquery.skyview.SkyView()\n",
    "\n",
    "# Note: skyview.list_surveys() shows the various image sources.\n",
    "\n",
    "# This takes a while\n",
    "im = skyview.get_images(ngc2419_pos,survey=['SDSSg'],radius=20.0*u.arcmin)\n",
    "pl.imshow(im[0][0].data,cmap='magma',origin='lower',vmax=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want numbers in degrees (rather than pixel numbers) on the axes. We also want to make sure the sky projection is correct (after all, these pixels we're looking at are projections onto a spherical surface). The difficult business of projecting stuff onto the sphere of the sky is dealt with by a definition standard called WCS, which stands for World Coordinate System. Ask your supervisor to explain what WCS is and why it's important (if your supervisor is a theorist, you may get a blank look...). The basic idea is that most astronomical images (that come from professional observatories) carry around WCS information in their FITS headers that software (like ds9) can use to project their CCD pixels properly onto your screen with accurate coordinate axes (accounting both for where the telescope is pointing on the sphere of the sky and for the optical distortion between the sky and the CCD plane). \n",
    "\n",
    "Fortunately, even though the details of WCS itself can be tricky to understand, you might never need to. We can just use the `astropy.wcs` module to get a WCS definition from the header of the image we downloaded in the previous cell, and use that to define all the projections we'll make on the subsequent plot.\n",
    "\n",
    "That means that if we want to plot a point with a given (RA,DEC) on top of our image, we first have to use the WCS projection to convert that (RA,DEC) to an (X,Y) position on our image. The WCS module in `astropy` has a function `all_world2pix` to do this (examples will follow below). For example, in the simplest common case, this would just do a tangent-plane projection of our spherical coordinate onto the plane of the image.\n",
    "\n",
    "Before we do any of that, if you've never seen a FITS header before you can execute the following cell. The entries up to the first line starting COMMENT are the WCS portion of the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im[0][0].header # Just to show you what a FITS header (with WCS keywords) looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import astropy\n",
    "pl.figure(figsize=(5,5))\n",
    "\n",
    "# Grab the WCS header from our image and use that to project everything else\n",
    "h   = im[0][0]\n",
    "wcs = astropy.wcs.WCS(h.header)\n",
    "\n",
    "# This is where we tell matplotlib about the WCS projection we want.\n",
    "pl.subplot(111,projection=wcs)\n",
    "\n",
    "# Plot the image on the axes we set up with the WCS projection\n",
    "pl.imshow(im[0][0].data,cmap='magma',origin='lower',vmax=0.5,alpha=0.5)\n",
    "\n",
    "# Use the WCS to convert the coordinates of the points to the pixel\n",
    "# coordinates of the image. We have to convert the RA and DEC to\n",
    "# astropy Angle objects to use them here.\n",
    "rapix, decpix = wcs.all_world2pix(c.Angle(res['ra'],u.degree),c.Angle(res['dec'],u.degree),1)\n",
    "\n",
    "# Plot the projected points\n",
    "pl.scatter(rapix,decpix,s=1,c='k',edgecolor='None');\n",
    "pl.grid(True,color='k')\n",
    "\n",
    "# Mark the cluster with a dot.\n",
    "# .to_pixel(wcs) is another way to use WCS to map a SkyCoord to a pixel\n",
    "x,y = ngc2419_pos.to_pixel(wcs)\n",
    "pl.scatter(x,y,c='r',s=25)\n",
    "\n",
    "# Draw a label for this point using annotate, a slightly complicated\n",
    "# function from matplotlib.\n",
    "pl.annotate('NGC 2419',(x,y),(x+100,y-100),color='k',fontsize=10,\n",
    "            arrowprops=dict(arrowstyle=\"-\", connectionstyle='arc'),\n",
    "            bbox=dict(facecolor='white', edgecolor='k',boxstyle='round'));\n",
    "\n",
    "# Store the axes object, we'll need it in a moment\n",
    "ax = pl.gca()\n",
    "\n",
    "# Plot a circle of radius 5 arcmin around the cluster. \n",
    "\n",
    "# From the astropy documentation on SphericalCircle:\n",
    "# \"This class is needed in cases where the user wants to add a circular patch\n",
    "# to a celestial image, since otherwise the circle will be distorted, because \n",
    "# a fixed interval in longitude corresponds to a different angle on the sky \n",
    "# depending on the latitude.\"\n",
    "from astropy.visualization.wcsaxes import SphericalCircle\n",
    "my_circle = SphericalCircle((ngc2419_pos.ra,ngc2419_pos.dec),\n",
    "                      5.0*u.arcmin, edgecolor='k', facecolor='None',\n",
    "                      transform=ax.get_transform('icrs'))\n",
    "\n",
    "# Put the circle onto the axes.\n",
    "ax.add_patch(my_circle)\n",
    "\n",
    "# Finally format the plot:\n",
    "\n",
    "# See here for how to format the tick marks\n",
    "# http://wcsaxes.readthedocs.io/en/latest/ticks_labels_grid.html\n",
    "# In our case:\n",
    "lon, lat = ax.coords\n",
    "lon.set_major_formatter('d.dd')\n",
    "lat.set_major_formatter('d.dd') # d.dd means degrees with 2 decimal places\n",
    "\n",
    "lon.set_ticks(spacing=5.0 * u.arcmin)\n",
    "lat.set_ticks(spacing=5.0 * u.arcmin)\n",
    "\n",
    "pl.xlabel(r'$\\alpha$ $(\\degree)$')\n",
    "pl.ylabel(r'$\\delta$ $(\\degree)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that east is to the left of this image, as it should be..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Finding points within a spherical angular distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notwithstanding the missing sources near the cluster, let's make a (rough!) colour magnitude diagram for the cluster stars. \n",
    "\n",
    "To do that we need to isolate points from the SDSS catalogue that are within some angular distance (in this case we'll take 5 arcmin) of the centre. The spherical geometry means we can't use the KDTree method from the last session directly.\n",
    "\n",
    "Astropy coordinate objects have a `separation` method that takes a list of coordinates and calculates their *angular* separation to that point. There are several associated routines to match catalogues (two big arrays of coordinates) by position, but I haven't included examples of those (these still use a KDTree behind the scenes, but they account for the spherical geometry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(4,4))\n",
    "\n",
    "# First make a colour magnitude diagram of all points\n",
    "rmag = res['psfMag_r']\n",
    "gmr  = res['psfMag_g']-res['psfMag_r']\n",
    "pl.scatter(gmr,rmag,s=1,c='k',edgecolor='None',label='All sources')\n",
    "pl.xlim(-1,2)\n",
    "pl.ylim(23,14)\n",
    "pl.xlabel('$g-r$',fontsize=12)\n",
    "pl.ylabel('$r$',fontsize=12)\n",
    "\n",
    "# Overplot objects classified as stars (SDSS type = 6)\n",
    "# (read the SDSS data model for more details about this type)\n",
    "is_sdss_star = res['type'] == 6\n",
    "pl.scatter(gmr[is_sdss_star],rmag[is_sdss_star],s=1,c='cyan',edgecolor='None',label='STAR class');\n",
    "\n",
    "# Get the ra and dec or all points in the SDSS data we downloaded\n",
    "photo_points  = c.SkyCoord(res['ra'],res['dec'],unit=u.degree)\n",
    "\n",
    "# FIND OBJECTS WITHIN 5 arcmin OF THE CLUSTER \n",
    "nearby        = ngc2419_pos.separation(photo_points).arcmin < 4\n",
    "nearby_points = np.where(nearby & is_sdss_star)[0]\n",
    "\n",
    "# Plot the colour and magnitude of these points in red\n",
    "pl.scatter(gmr[nearby_points],rmag[nearby_points],s=5,c='red',edgecolor='None',label='NGC 2419 ($<5$\")')\n",
    "pl.legend(loc='upper left',frameon=True,fontsize=9);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red points clearly show the line of the globular cluster's giant branch and horizontal branch. The cyan spike on the righthand side of the plot (around g-r = 1.5) is the metal-rich main sequence of Milky Way disk stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a rough estimate of the distance to the cluster from the fact that the horizontal branch has an absolute magnitude of about ~0.5 and on our plot above the apparent magnitude is r~20.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = 20.5 - 0.5\n",
    "distance_kpc = (10**((D + 5.0)/5.0)) / 1000.0\n",
    "print('Distance to NGC 2419 is roughly {} kpc'.format(distance_kpc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 SciServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you work a lot with SDSS (and in future, LSST and perhaps other similar surveys), you should get an account on [SciServer](http://www.sciserver.org/) rather than using `astroquery` (or similar) to download huge amounts of data. SciServer lets you work with SDSS data remotely rather than having to pull it over the web to your machine. The standard way to interact with SciServer is through Jupyter notebooks and ipython sessions, so this tutorial should have been good preparation for using it.\n",
    "\n",
    "You can create an account [here](http://portal.sciserver.org/login-portal/Account/Register), then follow the guide for getting started [here](http://compute.sciserver.org/dashboard/Home/Help) once you have the account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Cosmology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This](http://www.astropy.org/astropy-tutorials/edshift_plot.html) is a good introduction to the `astropy.cosmology` subpackage, for working with relations between redshift, distance, age, angular diameter, luminosity and so on, for different cosmological parameters.\n",
    "\n",
    "***Micro-challenge***: use the cosmology tools in astropy to plot the relation between redshift and the age of the universe for the first-year Planck cosmological parameters. Use the same tools to find the lookback time (t=0 at the present day) to redshift 3, the redshift at which the age of the universe is 4 Gyr, and the redshifts at which an angular scale of 1 arcsecond corresponds to a physical scale of 7 kpc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AstroML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a package that provides a lot of tools for data mining, Bayesian inference and machine learning, with a heavy  astronomy focus (sometimes this is called 'astrostatistics'). These sound like fancy buzzwords, but they're probably some of the most important techniques in astronomy in the next decade, for theorists as well as observers. The authors of AstroML and the accompanying book (see the website) include two of the most senior astronomers in the Large Synoptic Survey Telescope (LSST) project. \n",
    "\n",
    "A list of the topics covered is given in the [user guide](http://www.astroml.org/user_guide/index.html). The code to make the plots from the book is available at the website, but there is no tutorial -- they want you to read the book. Copies are available in the library.\n",
    "\n",
    "A closely related (more general, less directly astronomy-oriented) package is [`scikit-learn`](http://scikit-learn.org/stable/) (`astroML` itself builds on that package). \n",
    "\n",
    "The following example is reproduced (without permission, and with modification!) from the astroML package/website. It shows how to fit a line by maximizing the log likelihood using a built in astroML routine for doing exactly that (`linear_model.TLS_logL`).\n",
    "\n",
    "This example has an extra level of complexity -- rather than considering the x and y errors as independent, it includes a covariance between them that is different for each data point. It doesn't get much more complicated than this when fitting a straight line. You'll need to read the book for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# (Modified for the Durham Postgrad Astro course by Andrew Cooper)\n",
    "#\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "\n",
    "from scipy              import optimize\n",
    "from matplotlib         import pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "from astroML.linear_model  import TLS_logL\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "from astroML.datasets      import fetch_hogg2010test\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# These two functions are just to make the plot at the end, they're\n",
    "# not related to the fitting.\n",
    "\n",
    "def get_principal(sigma_x, sigma_y, rho_xy):\n",
    "    \"\"\"\n",
    "    Compute the ellipse pricipal axes and rotation from covariance.\n",
    "    \"\"\"\n",
    "    sigma_xy2 = rho_xy * sigma_x * sigma_y\n",
    "\n",
    "    alpha = 0.5 * np.arctan2(2 * sigma_xy2, (sigma_x ** 2 - sigma_y ** 2))\n",
    "    tmp1  = 0.5 * (sigma_x ** 2 + sigma_y ** 2)\n",
    "    tmp2  = np.sqrt(0.25 * (sigma_x ** 2 - sigma_y ** 2) ** 2 + sigma_xy2 ** 2)\n",
    "\n",
    "    return np.sqrt(tmp1 + tmp2), np.sqrt(tmp1 - tmp2), alpha\n",
    "\n",
    "def plot_ellipses(x, y, sigma_x, sigma_y, rho_xy, factor=2, ax=None):\n",
    "    \"\"\"\n",
    "    Plot ellipses (used to represent the data with covariant errors).\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    sigma1, sigma2, alpha = get_principal(sigma_x, sigma_y, rho_xy)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        ax.add_patch(Ellipse((x[i], y[i]),\n",
    "                             factor * sigma1[i], factor * sigma2[i],\n",
    "                             alpha[i] * 180. / np.pi,\n",
    "                             fc='none', ec='k'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data -- this is the same data we saw in session 2 for the straight line fit challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = fetch_hogg2010test()\n",
    "data = data[5:]  # no outliers\n",
    "\n",
    "x       = data['x']\n",
    "y       = data['y']\n",
    "sigma_x = data['sigma_x']\n",
    "sigma_y = data['sigma_y']\n",
    "rho_xy  = data['rho_xy']\n",
    "\n",
    "# Plot the data\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "ax  = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(x, y, c='k', s=9)\n",
    "plot_ellipses(x, y, sigma_x, sigma_y, rho_xy, ax=ax)\n",
    "ax.set_xlim(40, 250)\n",
    "ax.set_ylim(100, 600)\n",
    "ax.set_xlabel('$x$',fontsize=12)\n",
    "ax.set_ylabel('$y$',fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the code to do the maximum likelihood fit. You'll need to look at the book (section 8.8) to figure out the maths of what's going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find best-fit parameters.\n",
    "X  = np.vstack((x, y)).T\n",
    "dX = np.zeros((len(x), 2, 2))\n",
    "dX[:, 0, 0] = sigma_x ** 2\n",
    "dX[:, 1, 1] = sigma_y ** 2\n",
    "dX[:, 0, 1] = dX[:, 1, 0] = rho_xy * sigma_x * sigma_y\n",
    "\n",
    "# Minimize the log-likelihood\n",
    "min_func = lambda beta: -TLS_logL(beta, X, dX)\n",
    "beta_fit = optimize.fmin(min_func, x0=[-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally make the plot with the fit, assuming the cells above have been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A convienience function\n",
    "def get_m_b(beta):\n",
    "    \"\"\"\n",
    "    Translate between typical slope-intercept representation,\n",
    "    and the normal vector representation.\n",
    "    \"\"\"\n",
    "    b = np.dot(beta, beta) / beta[1]\n",
    "    m = -beta[0] / beta[1]\n",
    "    return m, b\n",
    "\n",
    "\n",
    "# Plot the data and fits\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax  = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(x, y, c='k', s=9)\n",
    "plot_ellipses(x, y, sigma_x, sigma_y, rho_xy, ax=ax)\n",
    "\n",
    "# Plot the best-fit line\n",
    "m_fit, b_fit = get_m_b(beta_fit)\n",
    "x_fit        = np.linspace(0, 300, 10)\n",
    "ax.plot(x_fit, m_fit * x_fit + b_fit, '-r')\n",
    "\n",
    "ax.set_xlim(40, 250)\n",
    "ax.set_ylim(100, 600)\n",
    "ax.set_xlabel('$x$',fontsize=12)\n",
    "ax.set_ylabel('$y$',fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a plot of the likelihood contours in the space of the two parameters `m` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax  = fig.add_subplot(111)\n",
    "\n",
    "def get_beta(m, b):\n",
    "    denom = (1 + m * m)\n",
    "    return np.array([-b * m / denom, b / denom])\n",
    "\n",
    "# Computing the log likelihood for a grid of (m,b):\n",
    "m    = np.linspace(1.7, 2.8, 100)\n",
    "b    = np.linspace(-60, 110, 100)\n",
    "logL = np.zeros((len(m), len(b)))\n",
    "\n",
    "for i in range(len(m)):\n",
    "    for j in range(len(b)):\n",
    "        logL[i, j] = TLS_logL(get_beta(m[i], b[j]), X, dX)\n",
    "\n",
    "# Plot the contours\n",
    "ax.contour(m, b, convert_to_stdev(logL.T),\n",
    "           levels=(0.683, 0.955, 0.997),\n",
    "           colors=['r','g','b'])\n",
    "ax.set_xlabel('slope',fontsize=12)\n",
    "ax.set_ylabel('intercept',fontsize=12)\n",
    "ax.set_xlim(1.7, 2.8)\n",
    "ax.set_ylim(-60, 110)\n",
    "\n",
    "# Plot the best-fit value\n",
    "pl.scatter(m_fit,b_fit,c='k',marker='s');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. HDF5 with `pytables` or `h5py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 is probably the best format for large binary datasets with complicated structure (better than fits, because FITS gets rather tricky for multidimensional array data, and for data split into many small groups). \n",
    "\n",
    "Many simulation codes (especially those favoured at Durham) output their results as HDF5. If you're saving more than `>100 Mb` of data, HDF5 is more practical than `pickle` or `numpy.savez`, and more portable.\n",
    "\n",
    "HDF5 files are collections of arrays (called 'datasets') arranged in a hierarchical structure a bit like a file system (rather than directories with files in them, there are 'groups' with datasets in them) -- you could also think of them as being similar to Python `dicts`. It's possible to associate 'attributes' ('headers', like comments or units) with individual datasets and to compress them to save disk space.\n",
    "\n",
    "Strangely, there are *two* almost equally good packages to deal with HDF5 in python: [`h5py`](http://www.h5py.org/) and [`pytables`](http://www.pytables.org/). The style of these packages is slightly different, with `pytables` being slightly closer to the way of working with hdf5 in `C` and focusing on high performance, and `h5py` being more abstract and closer to other python packages like `numpy`, focused on ease of use. The differences are described by the authors of `pytables` [here](http://www.pytables.org/FAQ.html#how-does-pytables-compare-with-the-h5py-project) and by the `h5py` team [here](http://docs.h5py.org/en/latest/faq.html#what-s-the-difference-between-h5py-and-pytables). For general use either is probably fine, because `pytables` is pretty easy to use now too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we need to install `h5py` and follow the [quick start page](http://docs.h5py.org/en/latest/quick.html) and [instructions for readoing and writing datasets](http://docs.h5py.org/en/latest/high/dataset.html) to show how to create an HDF5 file and read it back in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Fake data\n",
    "mass_dm    = np.random.random(100)\n",
    "mass_stars = np.random.random(100)\n",
    "age_stars  = np.random.random(100)\n",
    "\n",
    "# Write a file\n",
    "f = h5py.File('mydata.hdf5', mode='w')\n",
    "try: # (see following notes for an explanation of try..finally..)\n",
    "    f.create_dataset('/dm/mass',    data=mass_dm) # we give the 'path' to the data in each case\n",
    "    f.create_dataset('/stars/mass', data=mass_stars)\n",
    "    f.create_dataset('/stars/age',  data=age_stars)\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "# Read the same file back\n",
    "g = h5py.File('mydata.hdf5',mode='r')\n",
    "try:\n",
    "    stellar_mass_dataset = g['/stars/mass'] # This deliberately looks like accessing a dict object\n",
    "    \n",
    "    # As long as the file is open, the array can be accessed.\n",
    "    print('type of stellar mass dataset: {}'.format(type(stellar_mass_dataset)))\n",
    "    print('stellar_mass_dataset is {}'.format(stellar_mass_dataset))\n",
    "    print('First 10 elements of stellar mass: {}'.format(y[0:10]))\n",
    "    \n",
    "    # If you want to use the data after the file is closed,\n",
    "    # store its `value`\n",
    "    stellar_mass_values = stellar_mass_dataset.value[0:10]\n",
    "finally:\n",
    "    g.close();\n",
    "print(\"After we've closed the file...\")\n",
    "print('stellar_mass_values is {}'.format(stellar_mass_values))\n",
    "print('stellar_mass_dataset is {}'.format(stellar_mass_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 A note about `try...finally`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the `try..finally` block in that example? Remember that the point of the `try` block is to capture exceptions that happen inside it so you can do something with them rather than stopping teh code. The exceptions can be handled with an `except` clause (we saw this in week 1).\n",
    "\n",
    "***The code in the `finally` part of the block is *guaranteed* to be executed regardless of what happens in the `try..except..` part.***\n",
    "\n",
    "In the example above, we don't handle any exceptions with `except`, but we do use the `try` to make sure that the file is closed properly (with `f.close()`) even if something goes wrong.\n",
    "\n",
    "You don't *have* to do this, but in the past I've found it's possible to corrupt hdf5 files that are being written to if something goes wrong during the write and the file isn't properly closed. \n",
    "\n",
    "It's common to modify HDF5 files that already exist. This is one of the advantages of the 'directory-like' structure of HDF5: if you compute something based on the data in a file (for example, if you compute the magnitudes of a bunch of stars for which you already have fluxes) then it's very easy to store that information in the same file as a separate dataset (it's a real pain to do that with FITS). In such casess you want to be careful not to corrupt the file, so I always wrap the access to the file in a `try..finally` to be on the safe side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also try the same thing with `pytables` (tutorial [here](http://www.pytables.org/usersguide/tutorials.html)). This simple example looks very similar between the two packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables as tb # Note, not pytables\n",
    "\n",
    "# Fake data\n",
    "mass_dm    = np.random.random(100)\n",
    "mass_stars = np.random.random(100)\n",
    "age_stars  = np.random.random(100)\n",
    "\n",
    "f = tb.open_file('my_data.hdf5',mode='w')\n",
    "try:\n",
    "    # createparents=True makes sure the 'path' to the dataset exists\n",
    "    f.create_array('/dm',   'mass',obj=mass_dm,createparents=True)\n",
    "    f.create_array('/stars','mass',obj=mass_stars,createparents=True)\n",
    "    f.create_array('/stars','age', obj=age_stars,createparents=True)\n",
    "finally:\n",
    "    f.close()\n",
    "\n",
    "# Read the same file back\n",
    "g = tb.open_file('mydata.hdf5',mode='r')\n",
    "try:\n",
    "    # In pytables you can either use a object-like syntax for getting groups/datasets...\n",
    "    stellar_mass_dataset = g.root.stars.mass\n",
    "    # ... or this way, which looks more like pytables:\n",
    "    stellar_mass_dataset = g.get_node('/stars/mass')\n",
    "    \n",
    "    # As long as the file is open, the array can be accessed.\n",
    "    print('type of y: {}'.format(type(stellar_mass_dataset)))\n",
    "    print('stellar_mass_daaset is {}'.format(stellar_mass_dataset))\n",
    "    print('First 10 elements of stellar_mass_dataset: {}'.format(stellar_mass_dataset[0:10]))\n",
    "    \n",
    "    # In pytables, slicing the array gives the data directly\n",
    "    # To get the whole array, do values = dataset[:] or values = dataset.read()\n",
    "    stellar_mass_values = stellar_mass_dataset[0:10]\n",
    "finally:\n",
    "    g.close();\n",
    "print(\"After we've closed the file...\")\n",
    "print('stellar_mass_values is {}'.format(stellar_mass_values))\n",
    "print('stellar_mass_dataset is {}'.format(stellar_mass_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***(Not related to Python)*** There are two useful command line tools to work with HDF5 files: h5ls and h5dump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `h5ls` inspects the structure of a file (similar to the `ls` shell command)\n",
    "- `h5dump` shows the actual data.\n",
    "\n",
    "The examples below (which use shell commands, not IPython commands) assume that you've made the mydata.hdf5 file in the current directory using either of the examples above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These might not be installed on your system, if not then the following\n",
    "# shell commands will not work.\n",
    "\n",
    "!echo 'Output of h5ls:'\n",
    "!h5ls 'mydata.hdf5'\n",
    "!echo\n",
    "!echo 'Output of h5ls/stars:'\n",
    "!h5ls 'mydata.hdf5/stars'\n",
    "!echo\n",
    "!echo 'Output of h5dump -g /dm (to show the contents of the /dm group only)'\n",
    "!h5dump -g /dm 'mydata.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Reading binary files with `fromfile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ever need to read arrays directly from binary files (this is how simulation data used to be stored, before the likes of HDF5), then `numpy.fromfile` is the tool to use. `numpy` arrays have a method `tofile` that can be used in a similar way to write your own raw binary files if you ever need to, but in most cases you shouldn't need to: always use `numpy.save`, `hdf5` or some other portable, easy-to-use format for working with binary data. Unfortunately other people will not follow this advice, so you may still need to know how to read binary files.\n",
    "\n",
    "The `Fortran` language has a special `formatted` binary output, where each block of data is preceded by two bytes holding the length of the block in bytes, which I call the 'header'. An identical 'footer' is written at the end of the data block. The point of these is to allow you to move quickly through a binary file, because you only have to read the header blocks to 'skip' over the data blocks. The point of the footer is a consistency check that you've read the right number of data bytes. (Those of you who work with Gadget simulations might need to care about this one day, but hopefully no-one else will.) The following example writes such a file and then reads it again, to illustrate how `tofile` and `fromfile` work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make some data\n",
    "x = np.arange(0,100,dtype=np.float64)\n",
    "\n",
    "# Write data to a file\n",
    "with open('my_binary_file.dat','wb') as f:\n",
    "    np.array(len(x),dtype='i8').tofile(f) # Header byte \n",
    "    x.tofile(f)                           # The array\n",
    "    np.array(len(x),dtype='i8').tofile(f) # Footer byte\n",
    "\n",
    "# Read the file\n",
    "with open('my_binary_file.dat','rb') as f: # Open in binary read mode\n",
    "    block_length_start = np.fromfile(f,'i8',1)\n",
    "    data               = np.fromfile(f,'f8',block_length_start)\n",
    "    block_length_end   = np.fromfile(f,'i8',1)\n",
    "    assert(block_length_start == block_length_end) # Assert thrown an exception if its condition is false\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` is a general-purpose package for statistics and databases organized as tables. It's not all that widely used in astronomy (although it's very widely used elsewhere for 'data science') but it has some potentially useful functions. [Here](http://pandas.pydata.org/pandas-docs/version/0.15.2/overview.html) is an overview of what can be done with `pandas`. Some of the functions overlap with more astronomy-specific versions in `astropy` (for example, reading and manipulating data tables). I don't find `pandas` very easy to understand or use.\n",
    "\n",
    "There is a collection of `pandas` tutorials [here](http://pandas.pydata.org/pandas-docs/version/0.15.2/tutorials.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only reason I'm mentioning `pandas` is because it includes a fast function for matching lists of integers, a bit like the challenge in session 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import random\n",
    "import time \n",
    "\n",
    "### Setup\n",
    "\n",
    "na, nb = 10000, 1000000\n",
    "\n",
    "# Some arrays to match\n",
    "arr2  = np.array(random.sample(xrange(0,10*nb),nb))\n",
    "s     = random.sample(xrange(0,nb),na)\n",
    "arr1  = arr2[s]\n",
    "\n",
    "# Make sure we have a non-matching element\n",
    "arr1[0] = -1\n",
    "\n",
    "### Matching\n",
    "\n",
    "# We need an index value of the array order.\n",
    "idx = np.arange(0,len(arr2))\n",
    "\n",
    "t0_m = time.time() # time how long this takes\n",
    "\n",
    "# Make pandas dataframes with the two arrays\n",
    "frame_a = pandas.DataFrame({'xdata':arr1})\n",
    "frame_b = pandas.DataFrame({'xdata':arr2,'sidx':idx})\n",
    "\n",
    "# This is some magic with pandas that I probably found on StackOverflow.\n",
    "# This returns a dataframe, which we convert to array.\n",
    "matched_index = np.array(frame_a.xdata.map(frame_b.set_index('xdata').sidx))\n",
    "# Default dtype is float and 'no match' value is NaN.\n",
    "\n",
    "print(matched_index[0:3])\n",
    "\n",
    "# Put -1 where no match is found\n",
    "matched_index[np.isnan(matched_index)] = -1\n",
    "\n",
    "# Now we've fixed the NaN, convert the array to integers\n",
    "matched_index = np.array(matched_index,dtype=np.int64)\n",
    "t1_m = time.time()\n",
    "\n",
    "print(matched_index[0:3])\n",
    "print('Time to match {} values against {} values: {:f}s'.format(na,nb,t1_m-t0_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my laptop, that's about the same time as the `numpy`-based matched routine I wrote in the challenge solution `match.py`. You can use the `pandas` version if you don't want to write your own. The downsides are (i) I can't explain how exactly it works and (ii) you have to `import pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Healpy (using HealPix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`healpy` package](https://healpy.readthedocs.io/en/latest/) allows python to interact with [NASA's `HEALPIX` software](http://healpix.jpl.nasa.gov/). HEALPIX is a way of working with a clever equal-area pixelisation of the sphere, which is handy for anything involving statistics (or plots...) of distributions of points on the sky. It was invented mainly for work with CMB maps, but it is more widely useful for anything that's basically a histogram in spherical coordinates. Read the HEALPIX page for more details.\n",
    "\n",
    "Obviously, `HEALPIX` needs to be installed to use `healpy`. `pip install healpy` claims will install a copy of `HEALPIX` for you (see the installation instructions at the `healpy` link above).\n",
    "\n",
    "** Small digression about `conda` **\n",
    "\n",
    "`healpy` is not as straightforward to install with `conda` as the packages we've met so far, because it's not in the default location `conda` looks for packages on the web when you type something like `conda install healpy` (at least, not for the mac -- it might be for linux). The same was true for the `astroquery` package, but in that case we jumped straight to the solution (use the option `-c astropy` to select the astropy channel). What if we know there's a package called `healpy` somewhere, but we don't know what `conda` channel it's in?\n",
    "\n",
    "This is quite common, so it's worth mentioning how to solve it.  `conda` provides a chain of clues that get you to the right place in the end. The following (non-executable) cells illustrate how this worked on my system:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ conda install healpy\n",
    "\n",
    "Fetching package metadata .........\n",
    "\n",
    "\n",
    "PackageNotFoundError: Package not found: '' Package missing in current osx-64 channels: \n",
    "  - healpy\n",
    "\n",
    "You can search for packages on anaconda.org with\n",
    "\n",
    "    anaconda search -t conda healpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we follow that instruction:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ anaconda search -t conda healpy\n",
    "Using Anaconda API: https://api.anaconda.org\n",
    "Run 'anaconda show <USER/PACKAGE>' to get more details:\n",
    "Packages:\n",
    "     Name                      |  Version | Package Types   | Platforms      \n",
    "     ------------------------- |   ------ | --------------- | ---------------\n",
    "     CEFCA/healpy              |    1.9.1 | conda           | linux-64, linux-32, osx-64\n",
    "                                          : Healpix tools package for Python \n",
    "     OpenAstronomy/healpy      |    1.9.1 | conda           | linux-64, osx-64\n",
    "                                          : Healpix tools package for Python\n",
    "     conda-forge/healpy        |    1.9.1 | conda           | linux-64, osx-64\n",
    "                                          : Healpix tools package for Python\n",
    "     fermipy/healpy            |    1.9.1 | conda           | linux-64       \n",
    "                                          : Healpix tools package for Python\n",
    "     flomertens/healpy         |    1.9.1 | conda           | linux-64       \n",
    "                                          : Healpix tools package for Python\n",
    "     kadrlica/healpy           |    1.9.1 | conda           | linux-64       \n",
    "                                          : Healpix tools package for Python\n",
    "     lsst/lsst-healpy          |          | conda           | linux-64, osx-64\n",
    "     zonca/healpy              |    1.8.2 | conda           | linux-64       \n",
    "                                          : Healpix tools package for Python\n",
    "Found 8 packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of packages from non-default locations ('channels'). Some of these look promising -- `OpenAstronmy` sounds good, `fermipy` might sound familiar, as might `lsst`. Let's go with `OpenAstronomy` because it has `Astronomy` in the name (also it's what the `healpy` author reccomends, if you read the installation instructions for that package at the link above). The next step is given at the top of the above output: `Run 'anaconda show <USER/PACKAGE>' to get more details`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "anaconda show OpenAstronomy/healpy\n",
    "Using Anaconda API: https://api.anaconda.org\n",
    "Name:    healpy\n",
    "Summary: Healpix tools package for Python\n",
    "Access:  public\n",
    "Package Types:  conda\n",
    "Versions:\n",
    "   + 1.9.1\n",
    "\n",
    "To install this package with conda run:\n",
    "     conda install --channel https://conda.anaconda.org/OpenAstronomy healpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! The last line shows you how to actually install this package with `conda`.\n",
    "\n",
    "Once it's installed, you can look at the tutorial on the `healpy` page (link above). The following example shows a common use case, which is just to use `HEALPIX` as a way to make a histogram of points on the sky, exploiting the fact that each of the `HEALPIX` pixels has the same area. To do this, we first have to assign points to HEALPIX pixels (using `ang2pix`), then make a histogram of the pixel numbers (using `numpy`) and finally plot that histogram as a `HEALPIX` map.\n",
    "\n",
    "(`healpy` is slow to catch up with some of the more recent advances in matplotlib, so its plots look a bit old-fashioned sometimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import healpy as hp\n",
    "\n",
    "# Read the Sagittarius stream data from last week\n",
    "from astropy.table import Table\n",
    "d = Table(np.genfromtxt('./all_particles.txt',names=True,delimiter=','))\n",
    "\n",
    "# Galactic coordinates of stream stars\n",
    "l,b = d['l'].data, d['b'].data\n",
    "\n",
    "# These are ranges of galactic coordinates from the file\n",
    "print('Galactic coordinate ranges (degrees):')\n",
    "print(l.min(), l.max())\n",
    "print(b.min(), b.max())\n",
    "\n",
    "# Healpix has some particular conventions for angular coordinates\n",
    "# See http://healpix.jpl.nasa.gov/html/intronode6.htm\n",
    "print('HEALPIX coordinate ranges (radians/pi):')\n",
    "\n",
    "# theta is the healpix convention for *latitude*: [0,pi]\n",
    "theta = (b+90)*np.pi/180.0 \n",
    "print(theta.min()/np.pi,theta.max()/np.pi)\n",
    "# phi is the healpix convention for *longitude*: [0,2*pi)\n",
    "phi = l*np.pi/180.0\n",
    "print(phi.min()/np.pi,phi.max()/np.pi)\n",
    "\n",
    "# The resolution of healpix maps is determined by 'nside', \n",
    "# which must be a power of 2 (up to 2**30)\n",
    "nside = 2**6\n",
    "\n",
    "# If we want to know what the resolution is in square degrees,\n",
    "# healpy can tell us the pixel area:\n",
    "pixel_area = hp.nside2pixarea(nside,degrees=True)\n",
    "print('Area of nside={} pixels is {} sq. deg.'.format(nside,pixel_area))\n",
    "\n",
    "# First step: find the Healpix pixel for each star\n",
    "pix    = hp.ang2pix(nside,theta,phi,nest=True)\n",
    "\n",
    "npix   = hp.nside2npix(nside)   # Number of pixels for this value of nside\n",
    "bins   = np.arange(0,npix+1)    # Have to have exactly npix bins!\n",
    "\n",
    "# Make the histogram of pixel numbers\n",
    "h, _ = np.histogram(pix,bins=bins)  \n",
    "\n",
    "# Convert this to number per square degree\n",
    "h = h/pixel_area\n",
    "\n",
    "# Side note: \"_\" is a python convention; if something is assigned \n",
    "# to \"_\" it usually means you don't care about that thing; it's an \n",
    "# alternative to calling the variable 'junk' or something similar.\n",
    "# There is nothing deeply special about the name \"_\". Here we assign\n",
    "# the bins returned by histogram to _, because we already have them\n",
    "# in the variable 'bins'.\n",
    "\n",
    "# Plot a Mollweide projection of the density of stars\n",
    "cmap = pl.get_cmap('viridis')\n",
    "cmap.set_under('w') \n",
    "hp.mollview(h,rot=(90.0,0,0),nest=True,coord='G',cmap=cmap,\n",
    "            min=0,max=25)\n",
    "\n",
    "# Add lines\n",
    "hp.graticule(c='lightgrey')\n",
    "hp.visufunc.projtext(np.pi/2.0,0,'Galactic centre',color='w')\n",
    "hp.visufunc.projtext(np.pi/2.0,np.pi,'Anticentre',color='w')\n",
    "\n",
    "# Plot an 'orthographic' view\n",
    "cmap = pl.get_cmap('magma_r')\n",
    "cmap.set_under('w') \n",
    "hp.orthview(h,nest=True,cmap=cmap)\n",
    "hp.graticule(c='k')\n",
    "\n",
    "# Plot a Mollweide projection again, with graticule lines showing\n",
    "# RA and DEC coordinates\n",
    "hp.mollview(h,nest=True,coord='G',cmap='viridis',title='Galactic coordinates, equatorial graticule')\n",
    "hp.graticule(coord='E',c='lightgrey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all. Thanks for following the tutorial, and good luck with Python!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
